{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee1c0262",
   "metadata": {},
   "source": [
    "# Load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abaec212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.27.1\n"
     ]
    }
   ],
   "source": [
    "import requests # send request to website\n",
    "from bs4 import BeautifulSoup as bs # convert the web content to bs object\n",
    "from bs4 import Comment # search if we are caught by Amazon as a robot\n",
    "from fake_useragent import UserAgent #create fake user agent from different browser\n",
    "import re # regular expression\n",
    "import pandas as pd # output dataframe\n",
    "import numpy as np # fast data manipulation\n",
    "import random # randomly use agent header for sending request\n",
    "print(requests.__version__)\n",
    "import os\n",
    "import csv\n",
    "from string import punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddefe45",
   "metadata": {},
   "source": [
    "# How to create headers for request\n",
    "1. Some Tutorials I used:\n",
    "    - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#comments-and-other-special-strings\n",
    "    - https://www.blog.datahut.co/post/web-scraping-best-practices-tips\n",
    "    - https://stackoverflow.com/questions/63305902/why-cant-i-scrape-amazon-products-by-beautifulsoup\n",
    "    - https://www.digitalocean.com/community/tutorials/scrape-amazon-product-information-beautiful-soup\n",
    "    - https://stackoverflow.com/questions/63615686/how-to-scrape-data-from-amazon-canada\n",
    "    - https://stackoverflow.com/questions/33138937/how-to-find-all-comments-with-beautiful-soup\n",
    "    - https://pypi.org/project/fake-useragent/\n",
    "    - https://github.com/jhnwr/scrape-amazon-reviews/blob/main/review-scraper.py\n",
    "    - https://www.fullstaxx.com/2021/05/23/multipage-scraping-amazon-python/\n",
    "    - https://github.com/sergioteula/python-amazon-paapi\n",
    "    \n",
    "2. Depends on where Amazon location you are scraping, you need to use different headers. The following are just 2 examples:\n",
    "\n",
    "    - For Amazon Canada: you use:\n",
    "\n",
    "    `headers = {\n",
    "        'content-type': 'text/html;charset=UTF-8',\n",
    "        'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "        'Accept-Language': 'en-US,en;q=0.8',\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    }`\n",
    "\n",
    "    - For Amazon Indian, you use:\n",
    "\n",
    "    `headers = {'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}`\n",
    "\n",
    "    - For Amazon UK, you use:\n",
    "    \n",
    "    `headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}`\n",
    "3. Here is a list of User-Agent strings for different browsers: https://www.useragentstring.com/pages/useragentstring.php\n",
    "4. I will use fake-useragent (pip3 install fake-useragent)to generate a list of fake user agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ccfd08",
   "metadata": {},
   "source": [
    "# Createa a list of fake User Agent to disguise our IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "511a8e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4688\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "There is a pretty useful third-party package called fake-useragent \n",
    "that provides a nice abstraction layer over user agents: https://pypi.org/project/fake-useragent/\n",
    "\n",
    "If you don't want to use the local data, you can use the external data source to retrieve the user-agents. \n",
    "#Set use_external_data to True:\n",
    "'''\n",
    "ua = UserAgent(browsers=[\"chrome\", \"edge\", \"internet explorer\", \"firefox\", \"safari\", \"opera\"])\n",
    "# I will generate a lsit of fake agent string\n",
    "user_agent_set = set()\n",
    "for _ in range(100000):\n",
    "    user_agent_set.add(ua.random)\n",
    "'''\n",
    "Create a list of UserAgent, so that we can alternate using them\n",
    "'''\n",
    "#Creater the corresponding headers\n",
    "header_list = []\n",
    "header = {\n",
    "    'content-type': 'text/html;charset=UTF-8',\n",
    "    'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "    'Accept-Language': 'en-US,en;q=0.8',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'\n",
    "}\n",
    "for user_agent in user_agent_set:\n",
    "    header[\"User-Agent\"] = user_agent\n",
    "    header_list.append(header)\n",
    "#Total unique fake agent string\n",
    "print(len(header_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f8dc3c",
   "metadata": {},
   "source": [
    "# Fetch data from website and create bs object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "162af8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to request data from the website\n",
    "import numpy as np\n",
    "reviewlist = []\n",
    "def get_soup(url, header_list, soup_attempts, request_attempts):\n",
    "    soup = 'No Data Returned'\n",
    "    for _ in range(soup_attempts):\n",
    "        request_count = 0\n",
    "        page = ''\n",
    "        notDenied = True\n",
    "        #changes our header every time\n",
    "        header = random.choice(header_list)\n",
    "        while page == '' and request_count <= request_attempts and notDenied:\n",
    "            try:\n",
    "                request_count += request_count\n",
    "                page = requests.get(url, headers=header, timeout=10)\n",
    "                soup = bs(page.content, \"lxml\")\n",
    "                '''If the page returns a message like To discuss automated access \n",
    "                    to Amazon data please contact api-services-support@amazon.com.\n",
    "                    We know we are denied access to the web page.\n",
    "                    In this case, lets try again using different header\n",
    "                '''\n",
    "                comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "                for comment in comments:\n",
    "                    if (\"api-services-support@amazon.com\" in comment):\n",
    "                        notDenied = False\n",
    "                        print('denied')\n",
    "                        soup = 'No Data Returned'\n",
    "                if (notDenied):\n",
    "                    return soup\n",
    "                #Break the while loop if everything goes well\n",
    "                break\n",
    "            except:\n",
    "                print(\"Connection refused by the server..\")\n",
    "                print(\"Let me sleep for 5 seconds\")\n",
    "                time.sleep(5)\n",
    "                print(\"Was a nice sleep, now let me continue...\")\n",
    "                continue\n",
    "    return soup\n",
    "\n",
    "#Define a function to get the price of a product\n",
    "def get_amazon_price(soup):\n",
    "    try:\n",
    "        price = soup.find_all('span.a-price')\n",
    "        print(int(price))\n",
    "        return int(price)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('didnt work')\n",
    "        price = 'Not Available'\n",
    "        return None\n",
    " \n",
    "linklist = []\n",
    "duplicates = []\n",
    "for x in range(2,3):\n",
    "    soup = get_soup(f'https://www.amazon.co.uk/s?k=heaphones&page={x}',\n",
    "                    header_list, soup_attempts = 20, request_attempts = 50)\n",
    "    \n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if 'keywords=heaphones' in href:\n",
    "            if 'offer-listing' not in href:\n",
    "                if'#customerReviews' not in href:\n",
    "                    duplicates.append(href)\n",
    "\n",
    "duplicates = [x.split('/ref')[0] for x in duplicates]\n",
    "\n",
    "for i in duplicates:\n",
    "    # Add to the new list\n",
    "    # only if not present\n",
    "    if i not in linklist:\n",
    "        linklist.append(i)\n",
    "\n",
    "finalList = ['https://www.amazon.co.uk' + s for s in linklist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = []\n",
    "for x in finalList:\n",
    "    soup = get_soup(x,header_list, soup_attempts = 20, request_attempts = 50)\n",
    "    spans = soup.find('span', attrs = {'class' : 'a-price-whole'})\n",
    "    if spans == None:\n",
    "        finalList.remove(x)\n",
    "        finalList = finalList\n",
    "        continue\n",
    "    price.append(spans.text.strip(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "523f9c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = []\n",
    "for x in finalList:\n",
    "    soup = get_soup(x,header_list, soup_attempts = 20, request_attempts = 50)\n",
    "    spans = soup.find('span',id =\"acrCustomerReviewText\", attrs = {'class' : 'a-size-base'})\n",
    "    if spans == None:\n",
    "        finalList.remove(x)\n",
    "        continue\n",
    "    review.append(spans.text.strip(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e2daa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['URLs', 'Price', '#Ratings']\n",
    "\n",
    "with open('links.csv', 'w', newline='') as csvfile:\n",
    "    file_is_empty = os.stat('links.csv').st_size == 0\n",
    "    writer = csv.writer(csvfile)\n",
    "    if file_is_empty:\n",
    "        writer.writerow(headers)\n",
    "    writer.writerows(zip(finalList, price, review))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
