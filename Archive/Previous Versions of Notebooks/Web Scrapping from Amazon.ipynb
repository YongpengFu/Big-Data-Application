{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee1c0262",
   "metadata": {},
   "source": [
    "# Load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abaec212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.28.1\n"
     ]
    }
   ],
   "source": [
    "import requests # send request to website\n",
    "from bs4 import BeautifulSoup as bs # convert the web content to bs object\n",
    "from bs4 import Comment # search if we are caught by Amazon as a robot\n",
    "from fake_useragent import UserAgent #create fake user agent from different browser\n",
    "import re # regular expression\n",
    "import pandas as pd # output dataframe\n",
    "import numpy as np # fast data manipulation\n",
    "import random # randomly use agent header for sending request\n",
    "import time #If access is denied, sleep 5s and then request again\n",
    "print(requests.__version__)\n",
    "import os\n",
    "import csv\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddefe45",
   "metadata": {},
   "source": [
    "# How to create headers for request\n",
    "1. Some Tutorials I used:\n",
    "    - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#comments-and-other-special-strings\n",
    "    - https://www.blog.datahut.co/post/web-scraping-best-practices-tips\n",
    "    - https://stackoverflow.com/questions/63305902/why-cant-i-scrape-amazon-products-by-beautifulsoup\n",
    "    - https://www.digitalocean.com/community/tutorials/scrape-amazon-product-information-beautiful-soup\n",
    "    - https://stackoverflow.com/questions/63615686/how-to-scrape-data-from-amazon-canada\n",
    "    - https://stackoverflow.com/questions/33138937/how-to-find-all-comments-with-beautiful-soup\n",
    "    - https://pypi.org/project/fake-useragent/\n",
    "    - https://github.com/jhnwr/scrape-amazon-reviews/blob/main/review-scraper.py\n",
    "    - https://www.fullstaxx.com/2021/05/23/multipage-scraping-amazon-python/\n",
    "    - https://github.com/sergioteula/python-amazon-paapi\n",
    "    \n",
    "2. Depends on where Amazon location you are scraping, you need to use different headers. The following are just 2 examples:\n",
    "\n",
    "    - For Amazon Canada: you use:\n",
    "\n",
    "    `headers = {\n",
    "        'content-type': 'text/html;charset=UTF-8',\n",
    "        'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "        'Accept-Language': 'en-US,en;q=0.8',\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    }`\n",
    "\n",
    "    - For Amazon Indian, you use:\n",
    "\n",
    "    `headers = {'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}`\n",
    "\n",
    "    - For Amazon UK, you use:\n",
    "    \n",
    "    `headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US; rv:1.8.0.8) Gecko/20061025 Firefox/1.5.0.8'}`\n",
    "    \n",
    "    \n",
    "3. Here is a list of User-Agent strings for different browsers: https://www.useragentstring.com/pages/useragentstring.php\n",
    "4. I will use fake-useragent (pip3 install fake-useragent)to generate a list of fake user agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ccfd08",
   "metadata": {},
   "source": [
    "# Fetch data from individual website using a list of fake User Agent to disguise our IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "511a8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_soup:\n",
    "    header = None\n",
    "    #When the class is initiated, a list of user agent will be generated\n",
    "    '''\n",
    "    There is a pretty useful third-party package called fake-useragent \n",
    "    that provides a nice abstraction layer over user agents: https://pypi.org/project/fake-useragent/\n",
    "\n",
    "    If you don't want to use the local data, you can use the external data source to retrieve the user-agents. \n",
    "    #Set use_external_data to True:\n",
    "    '''\n",
    "    def __init__(self, total_user_agent = 1000):\n",
    "        ua = UserAgent(browsers=[\"chrome\", \"edge\", \"internet explorer\", \"firefox\", \"safari\", \"opera\"])\n",
    "        # I will generate a lsit of fake agent string with total number of total_user_agent\n",
    "        self.user_agent_set = set()\n",
    "        # Set a cap for user_agent_set to prevent endless loop\n",
    "        while(len(self.user_agent_set)<total_user_agent and len(self.user_agent_set) < 4500):\n",
    "            self.user_agent_set.add(ua.random)\n",
    "    '''\n",
    "    Define the function to get contents from each page. \n",
    "    Each header_attempts will use the same header until it is caught by the weg server.\n",
    "    In each header_attempts, we will try request_attempts times to request contents until we get the right contents\n",
    "    '''\n",
    "    def get_individual_soup(self, url, header_attempts = 10, request_attempts = 10):\n",
    "        self.soup = 'No Data Returned'\n",
    "        for _ in range(header_attempts):\n",
    "            request_count = 0\n",
    "            page = ''\n",
    "            notDenied = True\n",
    "            # We want to keep using the same header if that one particular header is working\n",
    "            # We change it unless it is recognized and banned by Web server\n",
    "            if get_soup.header is None:\n",
    "                user_agent = random.choice(list(self.user_agent_set))\n",
    "                get_soup.header = {'content-type': 'text/html;charset=UTF-8',\n",
    "                'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "                'Accept-Language': 'en-US,en;q=0.8',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                \"User-Agent\": user_agent}\n",
    "\n",
    "            while page == '' and request_count < request_attempts and notDenied:\n",
    "                try:\n",
    "                    request_count += 1\n",
    "                    page = requests.get(url, headers=get_soup.header, timeout=10)\n",
    "                    self.soup = bs(page.content, \"lxml\")\n",
    "                    '''If the page returns a message like To discuss automated access \n",
    "                        to Amazon data please contact api-services-support@amazon.com.\n",
    "                        We know we are denied access to the web page.\n",
    "                        In this case, lets try again using different header\n",
    "                    '''\n",
    "                    comments = self.soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "                    for comment in comments:\n",
    "                        if (\"api-services-support@amazon.com\" in comment):\n",
    "                            notDenied = False\n",
    "                            get_soup.header = None\n",
    "                            self.soup = 'No Data Returned'\n",
    "                    if (notDenied):\n",
    "                        return self.soup\n",
    "                    #We are caught by Web server as a bot, break this while and try a new header\n",
    "                    break\n",
    "                except:\n",
    "                    get_soup.header = None\n",
    "                    print(\"Connection refused by the server..\")\n",
    "                    print(\"Let me sleep for 5 seconds\")\n",
    "                    time.sleep(5)\n",
    "                    print(\"Now I will use a different header to request data...\")\n",
    "                    #The server does not respond to our request, break this while and try a new header\n",
    "                    break\n",
    "        return self.soup\n",
    "    '''\n",
    "    Customer Reviews, including Product Star Ratings, \n",
    "    help customers to learn more about the product and decide whether it is the right product for them.\n",
    "    To calculate the overall star rating and percentage breakdown by star, we donâ€™t use a simple average. \n",
    "    Instead, our system considers things like how recent a review is and if the reviewer bought the item on Amazon. \n",
    "    It also analyses reviews to verify trustworthiness.\n",
    "    Learn more from\n",
    "    https://www.amazon.co.uk/gp/help/customer/display.html/ref=cm_cr_arp_d_omni_lm_btn?nodeId=G8UYX7LALQC8V9KA'''\n",
    "    #Define a function to get the reciew of a product\n",
    "    def get_reviews(self, soup = None):\n",
    "        reviewlist = []\n",
    "        if soup is not None:\n",
    "            for item in soup.find_all('div', {'data-hook': 'review'}):\n",
    "                try:\n",
    "                    #This is domenstic review\n",
    "                    review = {\n",
    "                                'product Name': soup.title.text.replace('Amazon.co.uk:Customer reviews:', '').strip(),\n",
    "                                'Review Title': item.find('a', {'data-hook': 'review-title'}).get_text().strip(),\n",
    "                                'Review Rating':  float(item.find('i', {'data-hook': 'review-star-rating'}).get_text().replace('out of 5 stars', '').strip()),\n",
    "                                'Review Body': item.find('span', {'data-hook': 'review-body'}).get_text().strip(),\n",
    "                                'Review Date': item.find('span', {'data-hook': 'review-date'}).get_text().strip(),\n",
    "                                }\n",
    "                except AttributeError:\n",
    "                    #This is international review\n",
    "                    try:\n",
    "                        review = {\n",
    "                                'product Name': soup.title.text.replace('Amazon.co.uk:Customer reviews:', '').strip(),\n",
    "                                'Review Title': item.find('span', {'data-hook': 'review-title'}).get_text().strip(),\n",
    "                                'Review Rating':  float(item.find('i', {'data-hook': 'cmps-review-star-rating'}).get_text().replace('out of 5 stars', '').strip()),\n",
    "                                'Review Body': item.find('span', {'data-hook': 'review-body'}).get_text().strip(),\n",
    "                                'Review Date': item.find('span', {'data-hook': 'review-date'}).get_text().strip(),\n",
    "                                }\n",
    "                    except:\n",
    "                        #If there is still error, return None\n",
    "                        review = {\n",
    "                                'product Name': \"None\",\n",
    "                                'Review Title': \"None\",\n",
    "                                'Review Rating': \"None\",\n",
    "                                'Review Body': \"None\",\n",
    "                                'Review Date': \"None\",\n",
    "                                }\n",
    "                reviewlist.append(review)\n",
    "        return reviewlist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e488d0",
   "metadata": {},
   "source": [
    "# Example how you can iterate through each page to get the item link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73a55da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are on page 200\n",
      "You are using Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0) Opera 7.51  [en]to retrieve data\n",
      "You are on page 201\n",
      "You are using Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0) Opera 7.51  [en]to retrieve data\n",
      "You are on page 202\n",
      "You are using Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0) Opera 7.51  [en]to retrieve data\n",
      "You are on page 203\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 204\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 205\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 206\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 207\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 208\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 209\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 210\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 211\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 212\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 213\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 214\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 215\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 216\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 217\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 218\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 219\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 220\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 221\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 222\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 223\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 224\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 225\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 226\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 227\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 228\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 229\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 230\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 231\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 232\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 233\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 234\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 235\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 236\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 237\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 238\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 239\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 240\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 241\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 242\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 243\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 244\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 245\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 246\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 247\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 248\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n",
      "You are on page 249\n",
      "You are using Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/532.0 (KHTML, like Gecko) Chrome/3.0.197.11 Safari/532.0to retrieve data\n"
     ]
    }
   ],
   "source": [
    "# Get the link for each product in the home page\n",
    "mySoup = get_soup()\n",
    "#Grab the item link from each page and save them in a text file\n",
    "item_link = []\n",
    "# root_url = \"https://www.amazon.ca/s?k=headphones&i=electronics&page=\"\n",
    "# root_url = \"https://www.amazon.in/s?k=headphones&page=\"\n",
    "root_url = \"https://www.amazon.co.uk/s?k=headphones&i=electronics&s=review-rank&page=\"\n",
    "\n",
    "for page_number in range(200,250):\n",
    "    print(f\"You are on page {page_number}\")\n",
    "    home_soup = mySoup.get_individual_soup(root_url+str(page_number),\n",
    "                                          header_attempts = 2, request_attempts = 1)\n",
    "    #If there is nothing return from the website, go to next page\n",
    "    if home_soup != 'No Data Returned':\n",
    "        if (mySoup.header is not None):\n",
    "            print(\"You are using \" + mySoup.header[\"User-Agent\"] + \" to retrieve data\")\n",
    "    else:\n",
    "        print(f\"No data returned. You are using `{mySoup.header}` to retrieve data\")\n",
    "        continue\n",
    "    for link in home_soup.select(\"h2 a.a-link-normal.s-underline-text.s-underline-link-text.s-link-style\"):\n",
    "        item_link.append(link['href'])\n",
    "\n",
    "with open (\"./Dataset/partial items link CA6.txt\", mode = \"wt\") as f:\n",
    "    for link in item_link:\n",
    "        f.write(link+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df9d9df",
   "metadata": {},
   "source": [
    "# Generate a csv of links to each of those items, the price and the #of reviews From Stu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79880a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a new soup object\n",
    "mySoup = get_soup()\n",
    "\n",
    "home_soup = mySoup.get_individual_soup(root_url+str(page_number),\n",
    "                                          header_attempts = 2, request_attempts = 1)\n",
    "\n",
    "linklist = []\n",
    "duplicates = []\n",
    "for x in range(2,3):\n",
    "    soup = mySoup.get_individual_soup(f'https://www.amazon.co.uk/s?k=heaphones&page={x}',\n",
    "                                          header_attempts = 2, request_attempts = 1)\n",
    "    \n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if 'keywords=heaphones' in href:\n",
    "            if 'offer-listing' not in href:\n",
    "                if'#customerReviews' not in href:\n",
    "                    duplicates.append(href)\n",
    "\n",
    "duplicates = [x.split('/ref')[0] for x in duplicates]\n",
    "\n",
    "for i in duplicates:\n",
    "    # Add to the new list\n",
    "    # only if not present\n",
    "    if i not in linklist:\n",
    "        linklist.append(i)\n",
    "\n",
    "finalList = ['https://www.amazon.co.uk' + s + '/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews' for s in linklist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef3125f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = []\n",
    "for x in finalList:\n",
    "    soup = mySoup.get_individual_soup(x, header_attempts = 2, request_attempts = 1)\n",
    "    spans = soup.find('span', attrs = {'class' : 'a-price-whole'})\n",
    "    if spans == None:\n",
    "        finalList.remove(x)\n",
    "        finalList = finalList\n",
    "        continue\n",
    "    price.append(spans.text.strip(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37008e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = []\n",
    "for x in finalList:\n",
    "    soup = mySoup.get_individual_soup(x, header_attempts = 2, request_attempts = 1)\n",
    "    spans = soup.find('span',id =\"acrCustomerReviewText\", attrs = {'class' : 'a-size-base'})\n",
    "    if spans == None:\n",
    "        finalList.remove(x)\n",
    "        continue\n",
    "    review.append(spans.text.strip(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0e5a407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['URLs', 'Price', '#Ratings']\n",
    "\n",
    "with open('links.csv', 'w', newline='') as csvfile:\n",
    "    file_is_empty = os.stat('links.csv').st_size == 0\n",
    "    writer = csv.writer(csvfile)\n",
    "    if file_is_empty:\n",
    "        writer.writerow(headers)\n",
    "    writer.writerows(zip(finalList, price, review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1026de1b",
   "metadata": {},
   "source": [
    "# Extract product information from a single product item\n",
    "There are a few product information we can get from a single product item.\n",
    "\n",
    "- The title of the item\n",
    "- The price of the item\n",
    "- The rating of the item\n",
    "- Reviews of the item -- what the following code snipt does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc1abbf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#get one item review informaiton\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mySoup \u001b[38;5;241m=\u001b[39m \u001b[43mget_soup\u001b[49m()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# review_url = \"https://www.amazon.co.uk/product-reviews/B0BN1BJ94B/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=2\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m review_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.amazon.co.uk/Apple-EarPods-with-Lightning-Connector/product-reviews/B01M1EEPOB/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_soup' is not defined"
     ]
    }
   ],
   "source": [
    "#get one item review informaiton\n",
    "mySoup = get_soup()\n",
    "review_url = \"https://www.amazon.co.uk/product-reviews/B0BN1BJ94B/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=2\"\n",
    "review_url = \"https://www.amazon.co.uk/Apple-EarPods-with-Lightning-Connector/product-reviews/B01M1EEPOB/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
    "home_soup = mySoup.get_individual_soup(review_url,header_attempts = 2, request_attempts = 1)\n",
    "mySoup.get_reviews(home_soup)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
