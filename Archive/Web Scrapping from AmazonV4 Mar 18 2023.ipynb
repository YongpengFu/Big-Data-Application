{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee1c0262",
   "metadata": {},
   "source": [
    "# Load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "abaec212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.28.1\n"
     ]
    }
   ],
   "source": [
    "import requests # send request to website\n",
    "from bs4 import BeautifulSoup as bs # convert the web content to bs object\n",
    "from bs4 import Comment # search if we are caught by Amazon as a robot\n",
    "from fake_useragent import UserAgent #create fake user agent from different browser\n",
    "import re # regular expression\n",
    "import pandas as pd # output dataframe\n",
    "import numpy as np # fast data manipulation\n",
    "import random # randomly use agent header for sending request\n",
    "import time #If access is denied, sleep 5s and then request again\n",
    "from collections import defaultdict #Used to declare a dictionary with emply \n",
    "print(requests.__version__)\n",
    "import os\n",
    "import csv\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddefe45",
   "metadata": {},
   "source": [
    "# How to create headers for request\n",
    "1. Some Tutorials I used:\n",
    "    - https://www.crummy.com/software/BeautifulSoup/bs4/doc/#comments-and-other-special-strings\n",
    "    - https://www.blog.datahut.co/post/web-scraping-best-practices-tips\n",
    "    - https://stackoverflow.com/questions/63305902/why-cant-i-scrape-amazon-products-by-beautifulsoup\n",
    "    - https://www.digitalocean.com/community/tutorials/scrape-amazon-product-information-beautiful-soup\n",
    "    - https://stackoverflow.com/questions/63615686/how-to-scrape-data-from-amazon-canada\n",
    "    - https://stackoverflow.com/questions/33138937/how-to-find-all-comments-with-beautiful-soup\n",
    "    - https://pypi.org/project/fake-useragent/\n",
    "    - https://github.com/jhnwr/scrape-amazon-reviews/blob/main/review-scraper.py\n",
    "    - https://www.fullstaxx.com/2021/05/23/multipage-scraping-amazon-python/\n",
    "    - https://github.com/sergioteula/python-amazon-paapi\n",
    "    \n",
    "2. Depends on where Amazon location you are scraping, you need to use different headers. The following are just 2 examples:\n",
    "\n",
    "    - For Amazon Canada: you use:\n",
    "\n",
    "    `headers = {\n",
    "        'content-type': 'text/html;charset=UTF-8',\n",
    "        'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "        'Accept-Language': 'en-US,en;q=0.8',\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    }`\n",
    "\n",
    "    - For Amazon Indian, you use:\n",
    "\n",
    "    `headers = {'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.89 Safari/537.36'}`\n",
    "\n",
    "    - For Amazon UK, you use:\n",
    "    \n",
    "    `headers = {\n",
    "    'User-Agent':'Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US; rv:1.8.0.8) Gecko/20061025 Firefox/1.5.0.8'}`\n",
    "    \n",
    "    \n",
    "3. Here is a list of User-Agent strings for different browsers: https://www.useragentstring.com/pages/useragentstring.php\n",
    "4. I will use fake-useragent (pip3 install fake-useragent)to generate a list of fake user agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ccfd08",
   "metadata": {},
   "source": [
    "# Fetch data from individual website using a list of fake User Agent to disguise our IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "511a8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a class to deal with web request and convert it to beautiful soup\n",
    "class get_soup:\n",
    "    header = None\n",
    "    #When the class is initiated, a list of user agent will be generated\n",
    "    '''\n",
    "    There is a pretty useful third-party package called fake-useragent \n",
    "    that provides a nice abstraction layer over user agents: https://pypi.org/project/fake-useragent/\n",
    "\n",
    "    If you don't want to use the local data, you can use the external data source to retrieve the user-agents. \n",
    "    #Set use_external_data to True:\n",
    "    '''\n",
    "    def __init__(self, total_user_agent = 1000):\n",
    "        ua = UserAgent(browsers=[\"chrome\", \"edge\", \"internet explorer\", \"firefox\", \"safari\", \"opera\"])\n",
    "        # I will generate a lsit of fake agent string with total number of total_user_agent\n",
    "        self.user_agent_set = set()\n",
    "        # Set a cap for user_agent_set to prevent endless loop\n",
    "        while(len(self.user_agent_set)<total_user_agent and len(self.user_agent_set) < 4500):\n",
    "            self.user_agent_set.add(ua.random)\n",
    "    '''\n",
    "    Define the function to get contents from each page. \n",
    "    Each header_attempts will use the same header until it is caught by the weg server.\n",
    "    In each header_attempts, we will try request_attempts times to request contents until we get the right contents\n",
    "    '''\n",
    "    def get_individual_soup(self, url, header_attempts = 10, request_attempts = 10):\n",
    "        self.soup = 'No Data Returned'\n",
    "        for _ in range(header_attempts):\n",
    "            request_count = 0\n",
    "            page = ''\n",
    "            notDenied = True\n",
    "            # We want to keep using the same header if that one particular header is working\n",
    "            # We change it unless it is recognized and banned by Web server\n",
    "            if get_soup.header is None:\n",
    "                user_agent = random.choice(list(self.user_agent_set))\n",
    "                get_soup.header = {'content-type': 'text/html;charset=UTF-8',\n",
    "                'Accept-Encoding': 'gzip, deflate, sdch',\n",
    "                'Accept-Language': 'en-US,en;q=0.8',\n",
    "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "                \"User-Agent\": user_agent}\n",
    "\n",
    "            while page == '' and request_count < request_attempts and notDenied:\n",
    "                try:\n",
    "                    request_count += 1\n",
    "                    page = requests.get(url, headers=get_soup.header, timeout=10)\n",
    "                    self.soup = bs(page.content, \"lxml\")\n",
    "                    '''If the page returns a message like To discuss automated access \n",
    "                        to Amazon data please contact api-services-support@amazon.com.\n",
    "                        We know we are denied access to the web page.\n",
    "                        Or,\n",
    "                        Amazon page blocks you by returning a login page\n",
    "                        In either case, lets try again using different header\n",
    "                    '''\n",
    "                    comments = self.soup.find_all(string=lambda text: isinstance(text, Comment))\n",
    "                    login_page = self.soup.find('a', id = 'createAccountSubmit', class_ = 'a-button-text')                    \n",
    "                    for comment in comments:\n",
    "                        if (\"api-services-support@amazon.com\" in comment) or login_page:\n",
    "                            notDenied = False\n",
    "                            get_soup.header = None\n",
    "                            self.soup = 'No Data Returned'\n",
    "                            break\n",
    "                            \n",
    "                    if (notDenied):\n",
    "                        return self.soup\n",
    "                    #We are caught by Web server as a bot, break this while and try a new header\n",
    "                    break\n",
    "                except:\n",
    "                    get_soup.header = None\n",
    "                    print(\"Connection refused by the server..\")\n",
    "                    print(\"Let me sleep for 5 seconds\")\n",
    "                    time.sleep(5)\n",
    "                    print(\"Now I will use a different header to request data...\")\n",
    "                    #The server does not respond to our request, break this while and try a new header\n",
    "                    break\n",
    "        return self.soup\n",
    "    '''\n",
    "    Customer Reviews, including Product Star Ratings, \n",
    "    help customers to learn more about the product and decide whether it is the right product for them.\n",
    "    To calculate the overall star rating and percentage breakdown by star, we don’t use a simple average. \n",
    "    Instead, our system considers things like how recent a review is and if the reviewer bought the item on Amazon. \n",
    "    It also analyses reviews to verify trustworthiness.\n",
    "    Learn more from\n",
    "    https://www.amazon.co.uk/gp/help/customer/display.html/ref=cm_cr_arp_d_omni_lm_btn?nodeId=G8UYX7LALQC8V9KA'''\n",
    "    #Define a function to get the review of a product on one page only\n",
    "    def get_page_reviews(self, ASIN, soup = None):\n",
    "        reviewlist = []\n",
    "        if soup is not None:\n",
    "            for item in soup.find_all('div', {'data-hook': 'review'}):\n",
    "                try:\n",
    "                    #This is domenstic review\n",
    "                    review = {\n",
    "                                'ASIN': ASIN,\n",
    "                                'product Name': soup.title.text.replace('Amazon.co.uk:Customer reviews:', '').strip(),\n",
    "                                'Review Title': item.find('a', {'data-hook': 'review-title'}).get_text().strip(),\n",
    "                                'Review Rating':  float(item.find('i', {'data-hook': 'review-star-rating'}).get_text().replace('out of 5 stars', '').strip()),\n",
    "                                'Review Body': item.find('span', {'data-hook': 'review-body'}).get_text().strip(),\n",
    "                                'Review Date': item.find('span', {'data-hook': 'review-date'}).get_text().strip(),\n",
    "                                }\n",
    "                except AttributeError:\n",
    "                    #This is international review\n",
    "                    try:\n",
    "                        review = {\n",
    "                                'ASIN': ASIN,\n",
    "                                'product Name': soup.title.text.replace('Amazon.co.uk:Customer reviews:', '').strip(),\n",
    "                                'Review Title': item.find('span', {'data-hook': 'review-title'}).get_text().strip(),\n",
    "                                'Review Rating':  float(item.find('i', {'data-hook': 'cmps-review-star-rating'}).get_text().replace('out of 5 stars', '').strip()),\n",
    "                                'Review Body': item.find('span', {'data-hook': 'review-body'}).get_text().strip(),\n",
    "                                'Review Date': item.find('span', {'data-hook': 'review-date'}).get_text().strip(),\n",
    "                                }\n",
    "                    except:\n",
    "                        #If there is still error, return None\n",
    "                        review = {\n",
    "                                'ASIN': None,\n",
    "                                'product Name': None,\n",
    "                                'Review Title': None,\n",
    "                                'Review Rating': None,\n",
    "                                'Review Body': None,\n",
    "                                'Review Date': None,\n",
    "                                }\n",
    "                reviewlist.append(review)\n",
    "        return reviewlist\n",
    "\n",
    "#Create a class to handle all the file I/O\n",
    "class Review_file_io:\n",
    "    '''\n",
    "    This method is to get the root link for each product\n",
    "    '''\n",
    "    @classmethod\n",
    "    def get_review_link(cls, file_loc):\n",
    "        #Get the review entrance link for all the product items\n",
    "        review_links = {}\n",
    "        with open (file_loc, mode = \"r\") as f:\n",
    "            for link in f:\n",
    "                entry_link = link.strip().split(\",\")[0]\n",
    "                if (not re.search(\"product-reviews/.*/ref\", entry_link)):\n",
    "                    continue\n",
    "                ASIN = re.search(\"product-reviews/.*/ref\", entry_link).group(0).split(\"/\")[1]\n",
    "                '''Need to think this again, this is mainly for empty page loc'''\n",
    "                if re.search(r'&pageNumber=\\d+$', entry_link):\n",
    "                    review_links[ASIN] = entry_link\n",
    "                else:\n",
    "                    review_links[ASIN] = entry_link + \"&pageNumber=\"\n",
    "        return review_links\n",
    "    '''\n",
    "    This method is to get all the reviews on every page of a product\n",
    "    '''\n",
    "    def get_product_reviews(self, file_loc, reviews_loc, empty_page_loc, total_page = 999, header_attempts = 3, request_attempts = 1):\n",
    "        review_links = Review_file_io.get_review_link(file_loc)\n",
    "        mySoup = get_soup()\n",
    "        empty_page = defaultdict(list)\n",
    "        reviews = []\n",
    "        #loop through each page and get reviews on each page\n",
    "        for ASIN, review_link in review_links.items():\n",
    "            for page_number in range(1,total_page):\n",
    "                print(f\"You are on product {ASIN} page {page_number}\")\n",
    "                page_url = f\"{review_link}{page_number}\"\n",
    "                page_soup = mySoup.get_individual_soup(page_url,header_attempts = header_attempts, request_attempts = request_attempts)\n",
    "                '''\n",
    "                There are 3 cases page_soup equals 'No Data Returned'.\n",
    "                1st is when you get caught by Amazon as a bot;\n",
    "                2nd is Amazon returns you a login page\n",
    "                3rd is when our scrapper has tried header_attempts*request_attempts times to reach the page,\n",
    "                    but still got nothing, either rejected or caught by the server;\n",
    "\n",
    "                There are case that you do get the page content from our web scrapper,\n",
    "                but there are no reviews on that page. For example, \n",
    "                1. You get the page, but the page \n",
    "                2. you hit the last review page;\n",
    "                3. the product item just does not have any reviews at all.\n",
    "                '''\n",
    "                if page_soup != 'No Data Returned':\n",
    "                    review = mySoup.get_page_reviews(ASIN, page_soup)\n",
    "                    #There are simply no reviews for this product item, there are 2 things can happen:\n",
    "                    #1st: the review page is just some random page returned by Amazon\n",
    "                    #2nd: the review page is a normal review page but \n",
    "                        #because the page number has gone out of bound, there is simply no review at all\n",
    "                    if not review:\n",
    "                        #this is is to check if the page is a normal review page but the page number is out of boundary\n",
    "                        #The first find is to check if the page still has product title\n",
    "                        #The second find is to check if there is no Previous Page or Next Page button, that means this is it, there is no more reviews to look, break it\n",
    "                        #what is inside this tag is: '←Previous pageNext page→'\n",
    "                        if page_soup.find(\"a\", attrs={\"data-hook\": \"product-link\"}) and not page_soup.find(\"ul\", {'class': 'a-pagination'}):\n",
    "                            break\n",
    "                        continue\n",
    "                        \n",
    "                    reviews.extend(review)\n",
    "\n",
    "                    #if not page_soup.find(\"ul\", {'class': 'a-pagination'}):\n",
    "                        #break\n",
    "                    #Last page is hit, we break the for loop\n",
    "                    if page_soup.find('li', {'class': 'a-disabled a-last'}):\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "                #When we failed to get the content for this page, record this page, and go to the next page\n",
    "                else:\n",
    "                    empty_page[ASIN].append(page_url)\n",
    "                    continue\n",
    "        #Save the reviews and empty page link\n",
    "        try:\n",
    "            with open (reviews_loc, mode = \"a\") as f:\n",
    "                csv_columns = ['ASIN', 'product Name', 'Review Title', 'Review Rating', 'Review Body', 'Review Date']\n",
    "                writer = csv.DictWriter(f, fieldnames=csv_columns)\n",
    "                writer.writeheader()\n",
    "                for prod_info in reviews:\n",
    "                    writer.writerow(prod_info)\n",
    "\n",
    "            with open (empty_page_loc, mode = \"a\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(['URLs', 'ASIN'])\n",
    "                for key, page in empty_page.items():\n",
    "                    for link in page:\n",
    "                        writer.writerow([link, key])\n",
    "        except:\n",
    "            print(\"I/O error\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e488d0",
   "metadata": {},
   "source": [
    "# Example how you can iterate through each page to get the item link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a55da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are on page 200\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 201\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 202\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 203\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 204\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 205\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 206\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 207\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 208\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 209\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 210\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 211\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 212\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 213\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 214\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 215\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 216\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 217\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 218\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 219\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 220\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 221\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 222\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 223\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 224\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 225\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 226\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 227\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 228\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 229\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 230\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 231\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 232\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 233\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 234\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 235\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 236\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 237\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 238\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 239\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 240\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 241\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 242\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 243\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 244\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 245\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 246\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 247\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 248\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n",
      "You are on page 249\n",
      "You are using Mozilla/4.0 (compatible; MSIE 5.5b1; Mac_PowerPC) to retrieve data\n"
     ]
    }
   ],
   "source": [
    "# Get the link for each product in the home page\n",
    "mySoup = get_soup()\n",
    "#Grab the item link from each page and save them in a text file\n",
    "item_link = []\n",
    "# root_url = \"https://www.amazon.ca/s?k=headphones&i=electronics&page=\"\n",
    "# root_url = \"https://www.amazon.in/s?k=headphones&page=\"\n",
    "root_url = \"https://www.amazon.co.uk/s?k=headphones&i=electronics&s=review-rank&page=\"\n",
    "\n",
    "for page_number in range(200,250):\n",
    "    print(f\"You are on page {page_number}\")\n",
    "    home_soup = mySoup.get_individual_soup(root_url+str(page_number),\n",
    "                                          header_attempts = 2, request_attempts = 1)\n",
    "    #If there is nothing return from the website, go to next page\n",
    "    if home_soup != 'No Data Returned':\n",
    "        if (mySoup.header is not None):\n",
    "            print(\"You are using \" + mySoup.header[\"User-Agent\"] + \" to retrieve data\")\n",
    "    else:\n",
    "        print(f\"No data returned. You are using `{mySoup.header}` to retrieve data\")\n",
    "        continue\n",
    "    for link in home_soup.select(\"h2 a.a-link-normal.s-underline-text.s-underline-link-text.s-link-style\"):\n",
    "        item_link.append(link['href'])\n",
    "\n",
    "with open (\"./Dataset/partial items link CA7.txt\", mode = \"wt\") as f:\n",
    "    for link in item_link:\n",
    "        f.write(link+\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df9d9df",
   "metadata": {},
   "source": [
    "# Generate a csv of links to each of those items, the price and the #of reviews From Stu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79880a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a new soup object\n",
    "mySoup = get_soup()\n",
    "\n",
    "# home_soup = mySoup.get_individual_soup(root_url+str(page_number),\n",
    "#                                           header_attempts = 2, request_attempts = 1)\n",
    "\n",
    "linklist = []\n",
    "duplicates = []\n",
    "Skipped_pages = []\n",
    "for x in range(2,10):\n",
    "    soup = mySoup.get_individual_soup(f'https://www.amazon.co.uk/s?k=heaphones&page={x}',\n",
    "                                          header_attempts = 2, request_attempts = 1)\n",
    "    \n",
    "    #If there is nothing return from the website, go to next page\n",
    "    if soup != 'No Data Returned':\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if 'keywords=heaphones' in href:\n",
    "                if 'offer-listing' not in href:\n",
    "                    if '#customerReviews' not in href:\n",
    "                        duplicates.append(href)\n",
    "    else:\n",
    "        print(f\"No data returned. You are using `{mySoup.header}` to retrieve data\")\n",
    "        Skipped_pages.append(x)\n",
    "        continue\n",
    "\n",
    "duplicates = [x.split('/ref')[0] for x in duplicates]\n",
    "duplicates = [x.split('?keywords')[0] for x in duplicates]\n",
    "for i in duplicates:\n",
    "    # Add to the new list\n",
    "    # only if not present\n",
    "    if i not in linklist:\n",
    "        linklist.append(i)\n",
    "\n",
    "finalList = ['https://www.amazon.co.uk' + s for s in linklist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef3125f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "price = []\n",
    "for x in finalList:\n",
    "    soup = mySoup.get_individual_soup(x, header_attempts = 2, request_attempts = 1)\n",
    "    spans = soup.find('span', attrs = {'class' : 'a-price-whole'})\n",
    "    if spans == None:\n",
    "        price.append('')\n",
    "    else:\n",
    "        price.append(spans.text.strip(punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37008e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.amazon.co.uk/Apple-EarPods-with-Lightning-Connector/dp/B01M1EEPOB',\n",
       " 'https://www.amazon.co.uk/Sony-WH-1000XM3-Wireless-Cancelling-Headphones-Black/dp/B07GDR2LYK',\n",
       " 'https://www.amazon.co.uk/Earphones-Blukar-Headphones-Sensitivity-Microphone-Silver/dp/B07QLWMDLC',\n",
       " 'https://www.amazon.co.uk/JVC-HA-L50-B-E-Lightweight-Headphones-Black/dp/B000I2J4S4',\n",
       " 'https://www.amazon.co.uk/Sony-MDR-ZX310AP-Foldable-Headphones-Smartphone-Metallic-Red/dp/B00I3LV3EU',\n",
       " 'https://www.amazon.co.uk/EarFun-Wireless-Bluetooth-Detection-Headphones-Matte-Black/dp/B088H7GMHZ',\n",
       " 'https://www.amazon.co.uk/Sony-MDR-EX15AP-Earphones-Smartphone-Control-Black/dp/B00I3LV1HE',\n",
       " 'https://www.amazon.co.uk/Betron-AX5-Headphones-Microphone-Smartphones-Black-Gold/dp/B0786S43W4',\n",
       " 'https://www.amazon.co.uk/JVC-Headphones-Earphones-Compatible-Samsung-Black/dp/B00ZAT03S0',\n",
       " 'https://www.amazon.co.uk/Isolating-Headphones-Microphone-Lightweight-Earphones/dp/B083J88QRS',\n",
       " 'https://www.amazon.co.uk/OneOdio-Bluetooth-Over-Ear-Headphones/dp/B0828S1TPM',\n",
       " 'https://www.amazon.co.uk/Soundcore-Microphones-Reduction-Waterproof-Earphones-Black/dp/B07SJR6HL3',\n",
       " 'https://www.amazon.co.uk/Bluetooth-Headphones-KVIDIO-Microphone-Lightweight-pink/dp/B09XMJC9BL',\n",
       " 'https://www.amazon.co.uk/Bluetooth-Headphones-Earphones-Cancelling-Waterproof-Black/dp/B09BZ64R7S',\n",
       " 'https://www.amazon.co.uk/Sennheiser-HD-206-Stereo-Headphone/dp/B01N7S0IPR',\n",
       " 'https://www.amazon.co.uk/Wireless-Headphones-Foldable-Headsets-Earbuds-Black-Gold/dp/B07T1KZQV9',\n",
       " 'https://www.amazon.co.uk/Sony-WH-CH710N-Cancelling-Headphones-Assistant-Black/dp/B086LLYK4S',\n",
       " 'https://www.amazon.co.uk/Headphones-Bluetooth-Earphones-Cancelling-Waterproof-Black/dp/B099D925ZD',\n",
       " 'https://www.amazon.co.uk/yobola-Headphones-Waterproof-Earphones-Bluetooth-T2-Pro-White/dp/B08K4Y4RMB',\n",
       " 'https://www.amazon.co.uk/Edifier-W820NB-Hybrid-Cancelling-Headphones-Black/dp/B09FF244QP',\n",
       " 'https://www.amazon.co.uk/Soundcore-Cancelling-Headphones-Multiple-Bluetooth-Black/dp/B08HMWZBXC',\n",
       " 'https://www.amazon.co.uk/Headphone-Microphone-Bluetooth-Compatible-Smartphones/dp/B09HHFM9SH',\n",
       " 'https://www.amazon.co.uk/Betron-Earphones-Headphones-Microphone-Control-Black/dp/B0B6WKHFLS',\n",
       " 'https://www.amazon.co.uk/DOBOPO-Bluetooth-Earphones-Headphones-Waterproof-White/dp/B0B4DH8Q4C',\n",
       " 'https://www.amazon.co.uk/Wireless-Headphones-Playtime-Bluetooth-Foldable-Grey/dp/B08ND51MX8',\n",
       " 'https://www.amazon.co.uk/Bluetooth-Headphone-Microphone-Cancelling-Earphones/dp/B09XVKPGCM',\n",
       " 'https://www.amazon.co.uk/RockPapa-Adjustable-Headphones-Earphones-Headphone-Blue/dp/B0144PY56Q',\n",
       " 'https://www.amazon.co.uk/Bluetooth-Headphones-WorWoder-Memory-Protein-Travelling/dp/B07FS9B5G8',\n",
       " 'https://www.amazon.co.uk/PowerLocus-Bluetooth-Headphones-Headphone-Microphones-Red/dp/B0BLXRQ175',\n",
       " 'https://www.amazon.co.uk/Jabra-Bluetooth-Cancellation-technology-Multipoint-Titanium-Black/dp/B0B8DRY85Q',\n",
       " 'https://www.amazon.co.uk/Sony-MDR-ZX110NA-Overhead-Cancelling-Headphones-Black/dp/B00N3WWM58',\n",
       " 'https://www.amazon.co.uk/headphones-Microphone-Childrens-headsets-Green-black-green/dp/B07HHGP28X',\n",
       " 'https://www.amazon.co.uk/Headphones-Earbuds-HiFi-Audio-Isolating-Compatible-White-PD4/dp/B09W38W7MF',\n",
       " 'https://www.amazon.co.uk/Wireless-Headphones-Waterproof-Bluetooth-Earphones-White/dp/B09Z2WV32Q',\n",
       " 'https://www.amazon.co.uk/Headphones-Bluetooth-Earphones-Waterproof-Cancelling-Black/dp/B099YZTKCK',\n",
       " 'https://www.amazon.co.uk/Bluetooth-Headphones-Immersive-Earphones-Waterproof-Black/dp/B0B4RVBBRG',\n",
       " 'https://www.amazon.co.uk/Soundcore-Cancelling-Headphones-Bluetooth-Comfortable-Black/dp/B09FPCZ318',\n",
       " 'https://www.amazon.co.uk/Headphones-Earphones-Magnetic-Earbuds-Compatible/dp/B07ZNQYB27',\n",
       " 'https://www.amazon.co.uk/JBL-T110-Universal-Headphones-Control-Microphone-black/dp/B01MG62Z5M',\n",
       " 'https://www.amazon.co.uk/Blukar-Headphones-Sensitivity-Microphone-Multifunction-Black-Red/dp/B0B45WHNGL',\n",
       " 'https://www.amazon.co.uk/Earphones-Bluetooth-Headphones-Cancelling-Airp%F0%9D%96%94%F0%9D%96%89%F0%9D%96%98-Wireless-Earbuds/dp/B0BCZFBPSB',\n",
       " 'https://www.amazon.co.uk/Samsung-EHS64-3-5-Earphones-Remote-White/dp/B00GSPGJCE',\n",
       " 'https://www.amazon.co.uk/Betron-Earphones-Headphones-Powerful-Ergonomic-Black/dp/B01M2V05RE',\n",
       " 'https://www.amazon.co.uk/EO-EG920BW-Samsung-Handsfree-Headphone-Packaging-White/dp/B00ZLTBXSS',\n",
       " 'https://www.amazon.co.uk/Roxel-RX110-Lightweight-Headphones-Compatible-White/dp/B081NT4QC4',\n",
       " 'https://www.amazon.co.uk/Wireless-Bluetooth-Headphones-Charging-Waterproof-White/dp/B09NZMKF6W',\n",
       " 'https://www.amazon.co.uk/Bluetooth-Headphones-Rydohi-Lightweight-Black-Orange/dp/B07PCRLTRK',\n",
       " 'https://www.amazon.co.uk/Sony-MDRZX310L-AE-Foldable-Headphones-Metallic-Blue/dp/B00I3LUYNG',\n",
       " 'https://www.amazon.co.uk/Beats-Solo3-Wireless-Ear-Headphones/dp/B07YVXGFLS',\n",
       " 'https://www.amazon.co.uk/Bluetooth-Headphones-Cancelling-Microphone-Waterproof-White/dp/B0BB29LRQM',\n",
       " 'https://www.amazon.co.uk/Headphones-Bluetooth-Canceling-Waterproof-Earphones/dp/B0B2CZ6J51',\n",
       " 'https://www.amazon.co.uk/Tiksounds-headphones-Bluetooth-Waterproof-black/dp/B08LGRK6MT',\n",
       " 'https://www.amazon.co.uk/NXET-Headphone-Universal-Aluminum-Microsoft/dp/B01L00XVHC',\n",
       " 'https://www.amazon.co.uk/PowerLocus-Bluetooth-Over-Ear-Headphones-Wireless-White-Raspberry/dp/B07D3NPVLN',\n",
       " 'https://www.amazon.co.uk/Bluetooth-Headphones-Microphone-Earphones-Cancelling-Black/dp/B0B3X97WW1',\n",
       " 'https://www.amazon.co.uk/Headphones-Motast-Bluetooth-Earphones-Waterproof-New-Model/dp/B083M83HLD',\n",
       " 'https://www.amazon.co.uk/Headphones-Riwbox-XBT-80-Microphone-Black-Gold/dp/B073QV511F',\n",
       " 'https://www.amazon.co.uk/Panasonic-EAH-AZ70WE-K-Cancelling-Bluetooth-Functionality-Black/dp/B089BHKS6S',\n",
       " 'https://www.amazon.co.uk/Headphones-Bluetooth-Cancellation-Waterproof-Earphones-Wireless-Earbuds/dp/B0BMPY3NMM',\n",
       " 'https://www.amazon.co.uk/Genuine-Samsung-Earphones-Tuned-Line/dp/B071S9RFT7',\n",
       " 'https://www.amazon.co.uk/JKSWT-Microphone-Lightweight-Headphones-Compatible-White/dp/B09DY2N94D',\n",
       " 'https://www.amazon.co.uk/RIDER-Earphones-Headphones-Anti-Tangle-Microphone-Black/dp/B092MKW5CR',\n",
       " 'https://www.amazon.co.uk/Headphones-Microphone-earphone-Cancellation-Compatible-White-1pc/dp/B0BTWYHQ5H',\n",
       " 'https://www.amazon.co.uk/UMI-Bluetooth-headphones-earphones-intelligent-GREY/dp/B07N428SG9',\n",
       " 'https://www.amazon.co.uk/RockPapa-Bluetooth-Headphones-Microphone-Childrens-Purple/dp/B07C253NJ1',\n",
       " 'https://www.amazon.co.uk/TOZO-True-Wireless-Headphones-Stereo-Black/dp/B07RGZ5NKS',\n",
       " 'https://www.amazon.co.uk/Sumvision-Wave-RX-Efficiency-Cancellation-Black/dp/B07XRX18ZZ',\n",
       " 'https://www.amazon.co.uk/Headphones-Waterproof-Bluetooth-Earphones-Airp%F0%9D%96%94%F0%9D%96%89%F0%9D%96%98-White/dp/B0BD41NTX4',\n",
       " 'https://www.amazon.co.uk/Wireless-Bluetooth-Headphones-Rydohi-Foldable-Rose-Gold/dp/B07L93FRP7',\n",
       " 'https://www.amazon.co.uk/Sony-MDR-7506-Professional-Headphone-Black/dp/B000AJIF4E',\n",
       " 'https://www.amazon.co.uk/Sony-MDRRF811RK-CEK-RF811-Wireless-Headphones-Black/dp/B00JF3G0R0',\n",
       " 'https://www.amazon.co.uk/PowerLocus-Bluetooth-Headphones-Microphone-Lightweight-Purple-White/dp/B09312D2XL',\n",
       " 'https://www.amazon.co.uk/Earbuds-Headphones-Microphone-Earphones-Interface-4-Pairs/dp/B08B578J1P',\n",
       " 'https://www.amazon.co.uk/Headphones-Earphones-reduction-compatible-20%EF%BC%8CGoogle-TYC-White/dp/B0BMXHJGHK',\n",
       " 'https://www.amazon.co.uk/Beats-Studio3-Wireless-Cancelling-Headphones/dp/B08529BR4Q',\n",
       " 'https://www.amazon.co.uk/Bluetooth-Headphones-Cancelling-Microphone-Waterproof-Black/dp/B09PG3R55D',\n",
       " 'https://www.amazon.co.uk/Wireless-Bluetooth-Headphones-Cancelling-Earphones-White/dp/B09VDLS6KN',\n",
       " 'https://www.amazon.co.uk/Tribit-Bluetooth-Headphones-lightening-Cancelling-Black/dp/B086ZK4X35',\n",
       " 'https://www.amazon.co.uk/Panasonic-RB-HF420BE-K-Bluetooth-Headphones-Wireless-Black/dp/B08F8SQFT8',\n",
       " 'https://www.amazon.co.uk/Betron-Earphones-Headphones-Isolating-Microphone-Gold/dp/B074P5K78Q',\n",
       " 'https://www.amazon.co.uk/Earphones-Headphones-Microphone-Cancellation-Compatible-White-SG/dp/B09Z34JX14',\n",
       " 'https://www.amazon.co.uk/Headphones-Bluetooth-Cancelling-Earphones-Waterproof-Black/dp/B09V4J7Y7Z',\n",
       " 'https://www.amazon.co.uk/JKSWT-Lightweight-Microphone-Headphones-Compatible-White/dp/B0B2W9H7DW',\n",
       " 'https://www.amazon.co.uk/Betron-S2-Bluetooth-Headphones-Earphones-black/dp/B078C6FSN4',\n",
       " 'https://www.amazon.co.uk/Sony-WI-XB400-Extra-Wireless-Headphones-black/dp/B07X1TDTQB',\n",
       " 'https://www.amazon.co.uk/Lightning-Connector-Headphones-Certified-Isolating-1pc/dp/B09TR61Q26',\n",
       " 'https://www.amazon.co.uk/Sony-WH-XB910N-Cancelling-Wireless-Headphones-Black/dp/B09FKG4PP3',\n",
       " 'https://www.amazon.co.uk/DOQAUS-Wireless-Headphones-Bluetooth-Controller-Black/dp/B09FX97PPL',\n",
       " 'https://www.amazon.co.uk/JKSWT-Microphone-Definition-Headphones-Compatible-Black/dp/B0919BBRB2',\n",
       " 'https://www.amazon.co.uk/Sennheiser-Special-Open-Headphone-Black/dp/B07Q7S7247',\n",
       " 'https://www.amazon.co.uk/JVC-HA-S160-B-FLATS-Lightweight-Headphones-Black/dp/B004M7SQNU',\n",
       " 'https://www.amazon.co.uk/IeemTkxxe-Headphones-Earphones-Bluetooth-Cancellation-white/dp/B0B97VTX4Y',\n",
       " 'https://www.amazon.co.uk/Wireless-Bluetooth-Headphones-Earphones-Waterproof-Midnight-Black/dp/B0BFL83X8S',\n",
       " 'https://www.amazon.co.uk/Wireless-Headphones-Bluetooth-Canceling-Waterproof-New-Model/dp/B08JYZ38P7',\n",
       " 'https://www.amazon.co.uk/Betron-AX3-Headphones-Microphone-Smartphones-Black/dp/B0787JJ8J9',\n",
       " 'https://www.amazon.co.uk/Rockpapa-Comfort-Adjustable-Headphones-SmartPhones-Orange-Black/dp/B07DWNPNTC',\n",
       " 'https://www.amazon.co.uk/Urbanista-Bluetooth-Headphones-Cancelling-Microphone-Teal-Green/dp/B08TLV4XYJ',\n",
       " 'https://www.amazon.co.uk/Bluetooth-Headphones-Cancelling-Btootos-Waterproof-Black/dp/B0BCKHQGJN',\n",
       " 'https://www.amazon.co.uk/Earphones-Headphones-Sensitivity-Microphone-Definition-White/dp/B09KX53SGM',\n",
       " 'https://www.amazon.co.uk/Wireless-Certified-Bluetooth-Waterproof-Microphone-White-Wireless/dp/B0BJC4JBV3',\n",
       " 'https://www.amazon.co.uk/Bluetooth-Headphones-Wireless-Waterproof-Earphones-Black/dp/B091GHJ7FN',\n",
       " 'https://www.amazon.co.uk/Betron-Isolating-Headphones-Earphones-Powerful-Black/dp/B00MIQQVIY',\n",
       " 'https://www.amazon.co.uk/Headphones-Bluetooth-Cancellation-Waterproof-Earphones-white/dp/B0BFPQDXSG',\n",
       " 'https://www.amazon.co.uk/New-Headphones-Microphone-Children-Lightweight-Pink/dp/B09H4K1YR6',\n",
       " 'https://www.amazon.co.uk/Bluetooth-Wireless-Headphones-Louise-Mann-Black/dp/B08HRVFF65',\n",
       " 'https://www.amazon.co.uk/ZIHNIC-Bluetooth-Headphones-Over-Ear-Prolonged-rose/dp/B0BFFSRNGL',\n",
       " 'https://www.amazon.co.uk/Headphones-Reduction-bluetooth-earphones-Waterproof/dp/B0BKFXSM6Q',\n",
       " 'https://www.amazon.co.uk/Wireless-Headphones-Playtime-Bluetooth-Cellphone-Type1-Red/dp/B08FMMC1F8',\n",
       " 'https://www.amazon.co.uk/Tijjywwil-Earphones-Headphones-Microphone-Compatible-white/dp/B0B8S1WYZW',\n",
       " 'https://www.amazon.co.uk/Sony-Wireless-Cancelling-Headphones-Compatible-Blue/dp/B07X2T4QYP',\n",
       " 'https://www.amazon.co.uk/Philips-00-Headphones-Bluetooth-Isolation-Black/dp/B08CNM8YPD',\n",
       " 'https://www.amazon.co.uk/GNFROP-Environmental-Cancellation-Cancelling-Headphones-White/dp/B0BJ6Y28HY',\n",
       " 'https://www.amazon.co.uk/Groov-Headphones-Adjustable-Headphone-Smartphones-Pink/dp/B00378KLQK',\n",
       " 'https://www.amazon.co.uk/Sony-WH-CH520-Wireless-Bluetooth-Headphones-Beige/dp/B0BTJ9WHL9',\n",
       " 'https://www.amazon.co.uk/UGREEN-Headphones-Earphones-Earbuds-Compatible/dp/B094Y63W42',\n",
       " 'https://www.amazon.co.uk/Sony-WI-C310-Bluetooth-Wireless-Headphones-Black/dp/B07R37BSZ6',\n",
       " 'https://www.amazon.co.uk/Wireless-Headphones-Bluetooth-Playback-Waterproof-Rose-Gold/dp/B09YC24Q43',\n",
       " 'https://www.amazon.co.uk/ZJXD-Headphones-Microphone-Compatible-Smartphones-White/dp/B07PGD6FBW',\n",
       " 'https://www.amazon.co.uk/Jabra-Evolve2-Wireless-Headset-Long-Lasting-Black/dp/B08633D2K5',\n",
       " 'https://www.amazon.co.uk/C8-Headphones-Microphone-Lightweight-Smartphones-Mint/dp/B01EF5DBYM',\n",
       " 'https://www.amazon.co.uk/Earphones-Headphones-Microphone-Control-Compatible-White-SMA/dp/B0BN33Y2XD',\n",
       " 'https://www.amazon.co.uk/Satily-Earphones-Microphone-Cancelling-Waterproof-Black/dp/B0BTBHC1MM',\n",
       " 'https://www.amazon.co.uk/SONY-WI-C200-Wireless-Bluetooth-Headphones-Black/dp/B07QYWD718',\n",
       " 'https://www.amazon.co.uk/Samsung-Galaxy-Wireless-Earphones-Version/dp/B08C5HYHYB',\n",
       " 'https://www.amazon.co.uk/Bluetooth-Headphones-Playtime-Cancellation-Invisible-Ear/dp/B07YFMK89B']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = []\n",
    "for x in finalList:\n",
    "    soup = mySoup.get_individual_soup(x, header_attempts = 2, request_attempts = 1)\n",
    "    spans = soup.find('span',id =\"acrCustomerReviewText\", attrs = {'class' : 'a-size-base'})\n",
    "    if spans == None:\n",
    "        review.append('')\n",
    "    else:\n",
    "        review.append(spans.text.strip(punctuation))\n",
    "\n",
    "finalList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e5a407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = ['URLs', 'Price', '#Ratings']\n",
    "\n",
    "finalList = [s+ '/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews' for s in finalList]\n",
    "finalList = [s.replace(\"/dp/\", \"/product-reviews/\") for s in finalList]\n",
    "\n",
    "with open('links.csv', 'w', newline='') as csvfile:\n",
    "    file_is_empty = os.stat('links.csv').st_size == 0\n",
    "    writer = csv.writer(csvfile)\n",
    "    if file_is_empty:\n",
    "        writer.writerow(headers)\n",
    "    writer.writerows(zip(finalList, price, review))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed026a3",
   "metadata": {},
   "source": [
    "# Extract product information from multiple product items\n",
    "There are a few product information we can get from a single product item.\n",
    "\n",
    "- Product Name\n",
    "- Review Title\n",
    "- Review Rating\n",
    "- Review Body\n",
    "- Review Date\n",
    "\n",
    "Because each product has many pages of reviews and each product takes quite some time, I split the links.csv file to smaller files. Each file has about 35 links in there, and I will need other team members to work separately to reduce running time. https://phoenixnap.com/kb/linux-split#:~:text=The%20Linux%20split%20command%20breaks,Linux%20split%20command%20with%20examples.&text=Access%20to%20the%20terminal%20line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c30f5cc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# my_review = Review_file_io()\n",
    "# # my_review.get_review_link('./Dataset/Sample_link.csv')\n",
    "# my_review.get_product_reviews('./Dataset/Yong/best_seller_headphone_link.csv', './Dataset/Yong/best_review.csv', './Dataset/Yong/best_empty_link.csv', total_page = 200,\n",
    "#                              header_attempts=3, request_attempts=1)\n",
    "\n",
    "'''\n",
    "Concatnate multiple csv and remove duplicates, write to a parquet file\n",
    "'''\n",
    "lu_review = pd.read_csv(\"./Dataset/Yong/review2.csv\")\n",
    "total_review = pd.read_parquet(\"./Dataset/AmazonReviews.parquet\")\n",
    "AmazonReviews = pd.concat([total_review, lu_review], ignore_index=True)\n",
    "AmazonReviews.drop_duplicates().astype(str).to_parquet('./Dataset/AmazonReviews2.parquet')\n",
    "\n",
    "#The following is used to get Amazon Product Price informaiton\n",
    "link1 = pd.read_csv(\"./Dataset/links.csv\")\n",
    "link2 = pd.read_csv(\"./Dataset/links2.csv\")\n",
    "link1['ASIN'] = link1.apply(lambda x: re.search(\"product-reviews/.*/ref\", x['URLs']).group(0).split(\"/\")[1], axis=1)\n",
    "link2['ASIN'] = link2.apply(lambda x: re.search(\"product-reviews/.*/ref\", x['URLs']).group(0).split(\"/\")[1], axis=1)\n",
    "AmazonProductPrice = pd.concat([link1, link2], ignore_index=True).drop_duplicates(subset=\"ASIN\")\n",
    "AmazonProductPrice.to_csv('./Dataset/AmazonProductPrice.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed9ea74",
   "metadata": {},
   "source": [
    "# Extract home page information from best seller headphone pages\n",
    "There are a few product information we can get from a single product item.\n",
    "\n",
    "- ASIN\n",
    "- Product Name\n",
    "- Product Link\n",
    "- Product Rank\n",
    "- Product Price (if any)\n",
    "- Product total reviews\n",
    "\n",
    "I am only looking at the best seller headphone pages so that we can compare other headphones with them.\n",
    "There are only two pages to look at: \n",
    "\n",
    "- 'https://www.amazon.co.uk/Best-Sellers-Electronics-Photo-Headphones-Earphones/zgbs/electronics/4085731/ref=zg_bs_pg_2?_encoding=UTF8&pg=2'\n",
    "- 'https://www.amazon.co.uk/gp/bestsellers/electronics/4085731?ref_=Oct_d_obs_S&pd_rd_w=GN87T&content-id=amzn1.sym.6b376e0b-c3fa-4b95-b5c3-82d50c091b51&pf_rd_p=6b376e0b-c3fa-4b95-b5c3-82d50c091b51&pf_rd_r=04E83P9GTV047MBCJ2D8&pd_rd_wg=CKOyq&pd_rd_r=49c2d810-4b2c-4618-a164-7ed75ea5606a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "057f0688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "best_seller_headphone_homepage = ['https://www.amazon.co.uk/Best-Sellers-Electronics-Photo-Headphones-Earphones/zgbs/electronics/4085731/ref=zg_bs_pg_2?_encoding=UTF8&pg=2',\n",
    "                                 'https://www.amazon.co.uk/gp/bestsellers/electronics/4085731?ref_=Oct_d_obs_S&pd_rd_w=GN87T&content-id=amzn1.sym.6b376e0b-c3fa-4b95-b5c3-82d50c091b51&pf_rd_p=6b376e0b-c3fa-4b95-b5c3-82d50c091b51&pf_rd_r=04E83P9GTV047MBCJ2D8&pd_rd_wg=CKOyq&pd_rd_r=49c2d810-4b2c-4618-a164-7ed75ea5606a']\n",
    "mySoup = get_soup()\n",
    "first_page = mySoup.get_individual_soup(best_seller_headphone_homepage[0],header_attempts = 3, request_attempts = 1)\n",
    "second_page = mySoup.get_individual_soup(best_seller_headphone_homepage[1],header_attempts = 3, request_attempts = 1)\n",
    "\n",
    "best_seller_headphone_homepage_list=[]\n",
    "for i, title in enumerate(second_page.find_all(\"a\", class_ = \"a-link-normal\")):\n",
    "    if i%4 == 0:\n",
    "        Product_Link =\"https://www.amazon.co.uk\"+ title['href']\n",
    "        Product_Link = Product_Link.replace('dp', \"product-reviews\")\n",
    "        Product_Link = re.sub(\"ref.*\",\"\",Product_Link)+ \"ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"\n",
    "        ASIN = re.search(\"dp/.*/ref\", title['href']).group(0).split(\"/\")[1]\n",
    "    elif i%4==1:\n",
    "        Product_Name = title.get_text() \n",
    "    elif i%4==2:\n",
    "        Product_Rating = title.get_text().split(\" \")[0]\n",
    "        Product_total_reviews = title.get_text().split(\"stars\")[1].strip()\n",
    "    elif i%4==3:\n",
    "        Product_Price = title.get_text()\n",
    "        info_dict = {\"URL\": Product_Link,\n",
    "                     \"Price\":Product_Price,\n",
    "                     \"Rating\":Product_Rating,\n",
    "                     \"ASIN\":ASIN,\n",
    "                     \"Product_Name\":Product_Name,\n",
    "                     \"Total_Reviews\":Product_total_reviews\n",
    "                }\n",
    "        best_seller_headphone_homepage_list.append(info_dict)\n",
    "print(len(best_seller_headphone_homepage_list))\n",
    "with open (\"./Dataset/best_seller_headphone_link.csv\", mode = \"a\") as f:\n",
    "                csv_columns = ['URL', 'Price', 'Rating', 'ASIN', 'Product_Name', 'Total_Reviews']\n",
    "                writer = csv.DictWriter(f, fieldnames=csv_columns)\n",
    "                writer.writeheader()\n",
    "                for prod_info in best_seller_headphone_homepage_list:\n",
    "                    writer.writerow(prod_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429bbd5",
   "metadata": {},
   "source": [
    "# Text Analysis for the Product Review\n",
    "We will do a few different things here, in general we have:\n",
    "\n",
    "- Text Summarization: this is to summarize all the reviews associated with one product item to 5 maximum sentense. (https://www.activestate.com/blog/how-to-do-text-summarization-with-python/)\n",
    "- Sentiment Analysis to classify product item reviews\n",
    "    - https://realpython.com/sentiment-analysis-python/\n",
    "    - https://www.kaggle.com/code/paoloripamonti/twitter-sentiment-analysis\n",
    "    - https://www.kaggle.com/datasets/shitalkat/amazonearphonesreviews/code\n",
    "    - https://www.kaggle.com/code/foolwuilin/sentiment-analysis-for-3-earphones\n",
    "- To predict the Produtct Price using a range of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a2d70",
   "metadata": {},
   "source": [
    "## Understand the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "03985059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(213846, 6)\n",
      "238\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASIN</th>\n",
       "      <th>product Name</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>Review Rating</th>\n",
       "      <th>Review Body</th>\n",
       "      <th>Review Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000AJIF4E</td>\n",
       "      <td>Sony MDR-7506/1 Professional Headphone, Black ...</td>\n",
       "      <td>Superb audio, changes how you listen to music</td>\n",
       "      <td>5.0</td>\n",
       "      <td>These are just superb for audio quality. You'l...</td>\n",
       "      <td>Reviewed in the United Kingdom on 1 March 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000AJIF4E</td>\n",
       "      <td>Sony MDR-7506/1 Professional Headphone, Black ...</td>\n",
       "      <td>Basic build Great sound for the price</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Product used for purely listening purposes mai...</td>\n",
       "      <td>Reviewed in the United Kingdom on 7 January 2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ASIN                                       product Name  \\\n",
       "0  B000AJIF4E  Sony MDR-7506/1 Professional Headphone, Black ...   \n",
       "1  B000AJIF4E  Sony MDR-7506/1 Professional Headphone, Black ...   \n",
       "\n",
       "                                    Review Title Review Rating  \\\n",
       "0  Superb audio, changes how you listen to music           5.0   \n",
       "1          Basic build Great sound for the price           4.0   \n",
       "\n",
       "                                         Review Body  \\\n",
       "0  These are just superb for audio quality. You'l...   \n",
       "1  Product used for purely listening purposes mai...   \n",
       "\n",
       "                                        Review Date  \n",
       "0    Reviewed in the United Kingdom on 1 March 2023  \n",
       "1  Reviewed in the United Kingdom on 7 January 2023  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 238 product items in our dataset, with 213846 reviews in total.\n"
     ]
    }
   ],
   "source": [
    "#Load the data\n",
    "import pandas as pd #Used for manipulate dataset\n",
    "total_review = pd.read_parquet(\"./Dataset/AmazonReviews.parquet\")\n",
    "#There one addition header row that we dont need\n",
    "total_review = total_review.loc[~total_review[\"ASIN\"].isin([\"ASIN\"])]\n",
    "print(total_review.shape)\n",
    "print(len(total_review.ASIN.unique()))\n",
    "display(total_review.head(2))\n",
    "print(\"There are total 238 product items in our dataset, with 213846 reviews in total.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1a155f",
   "metadata": {},
   "source": [
    "### Summarizing Text With SpaCy\n",
    "Our assumption is that a higher-frequency word use implies a more ‘significant’ meaning. This may seem overly simplistic, but this approach often produces surprisingly good results. Based on this assumption, we will do the following steps: (https://www.activestate.com/blog/how-to-do-text-summarization-with-python/)\n",
    "- Look at the use frequency of specific words\n",
    "- Sum the frequencies within each sentence\n",
    "- Rank the sentences based on this sum\n",
    "\n",
    "We’ll use SpaCy to import a pre-trained NLP pipeline to help interpret the grammatical structure of the text. This will allow us to identify the most common words that are often useful to filter out (i.e. STOP_WORDS) as well as the punctuation (i.e. punctuation). We’ll also use the nlargest function to extract a percentage of the most important sentences. Our algorithm will use the following steps:\n",
    "\n",
    "- Tokenize the text with the SpaCy pipeline. This segments the text into words, punctuation, and so on, using grammatical rules specific to the English language. \n",
    "- Count the number of times a word is used (not including stop words or punctuation), then normalize the count. A word that’s used more frequently has a higher normalized count.\n",
    "- Calculate the sum of the normalized count for each sentence.\n",
    "- Extract a percentage of the highest ranked sentences. These serve as our summary.\n",
    "\n",
    "We will above so for each of the product item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "48ba972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy #used for text summarization\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "# !python -m spacy download en_core_web_sm\n",
    "#Define the function based on the above steps\n",
    "def summarize(text, select_length = 5):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.max_length = 2226047 # or even higher\n",
    "    doc= nlp(text)\n",
    "    tokens=[token.text for token in doc]\n",
    "    word_frequencies={}\n",
    "    for word in doc:\n",
    "        if word.text.lower() not in list(STOP_WORDS):\n",
    "            if word.text.lower() not in punctuation:\n",
    "                if word.text not in word_frequencies.keys():\n",
    "                    word_frequencies[word.text] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.text] += 1\n",
    "    max_frequency=max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word]=word_frequencies[word]/max_frequency\n",
    "    sentence_tokens= [sent for sent in doc.sents]\n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_tokens:\n",
    "        for word in sent:\n",
    "            if word.text.lower() in word_frequencies.keys():\n",
    "                if sent not in sentence_scores.keys():                            \n",
    "                    sentence_scores[sent]=word_frequencies[word.text.lower()]\n",
    "                else:\n",
    "                    sentence_scores[sent]+=word_frequencies[word.text.lower()]\n",
    "    summary = {k: v for k, v in sorted(sentence_scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "    return list(summary)[:select_length]\n",
    "#I think we need to use \" \" instead of \",\" to join reviews together?????\n",
    "ASIN_review = total_review.groupby(['ASIN'])['Review Body'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "ASIN_review[\"top 5\"] = ASIN_review['Review Body'].apply(summarize)\n",
    "ASIN_review.to_csv('./Dataset/AmazonReviewsSummarize2.csv', index = False)\n",
    "ASIN_review.astype(str).to_parquet('./Dataset/AmazonReviewsSummarize2.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1906e5",
   "metadata": {},
   "source": [
    "### Use Sentiment Analysis With Python to Classify Reviews (this method is abandoned because of complexity)\n",
    "Tutorials from https://realpython.com/sentiment-analysis-python/\n",
    "\n",
    "**Step 1**: Using Natural Language Processing to Preprocess and Clean Text Data\n",
    "- Tokenizing sentences to break text down into sentences, words, or other units\n",
    "- Removing stop words like “if,” “but,” “or,” and so on\n",
    "- Normalizing words by condensing all forms of a word into a single form\n",
    "- Vectorizing text by turning the text into a numerical representation for consumption by your classifie\n",
    "- `!python -m spacy download en_core_web_sm`: download its English Language model. spaCy supports a number of different languages, which are listed on the [spaCy website](https://spacy.io/usage/models).\n",
    "\n",
    "### Training your classifier\n",
    "Putting the spaCy pipeline together allows you to rapidly build and train a convolutional neural network (CNN) for classifying text data. While you’re using it here for sentiment analysis, it’s general enough to work with any kind of text classification task as long as you provide it with the training data and labels.\n",
    "\n",
    "In this part of the project, you’ll take care of three steps:\n",
    "\n",
    "1. Modifying the base spaCy pipeline to include the textcat component\n",
    "2. Building a training loop to train the textcat component\n",
    "3. Evaluating the progress of your model training after a given number of training loops\n",
    "\n",
    "First, you’ll add textcat to the default spaCy pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4d53fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tokenizing: \n",
    "Tokenization is the process of breaking down chunks of text into smaller pieces. \n",
    "spaCy comes with a default processing pipeline that begins with tokenization, \n",
    "making this process a snap. In spaCy, you can do either sentence tokenization or word tokenization:\n",
    "\n",
    "Word tokenization breaks text down into individual words.\n",
    "Sentence tokenization breaks text down into individual sentences.\n",
    "'''\n",
    "#note, you need to convert all the strings to lowercase, because stop words are in lower case\n",
    "ASIN_review2 = total_review.groupby(['ASIN'])['Review Body'].apply(lambda x: ' '.join(x.str.lower())).reset_index()\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#doc has type of spacy.tokens.doc.Doc\n",
    "doc = nlp(ASIN_review2.iloc[0,1])\n",
    "#token has type of spacy.tokens.token.Token\n",
    "token_list = [token for token in doc]\n",
    "\n",
    "'''\n",
    "Removing Stop Words:\n",
    "Stop words are words that may be important in human communication but are of little value for machines. \n",
    "spaCy comes with a default list of stop words that you can customize. \n",
    "For now, you’ll see how you can use token attributes to remove stop words:\n",
    "nlp.Defaults.stop_words will show all the stop words in spacy\n",
    "'''\n",
    "filtered_tokens = [token for token in doc if not token.is_stop]\n",
    "\n",
    "'''\n",
    "Normalization is a little more complex than tokenization. \n",
    "It entails condensing all forms of a word into a single representation of that word. \n",
    "For instance, “watched,” “watching,” and “watches” can all be normalized into “watch.” \n",
    "There are two major normalization methods:\n",
    "\n",
    "Stemming\n",
    "Lemmatization\n",
    "\n",
    "Because lemmatization is generally more powerful than stemming, \n",
    "it’s the only normalization strategy offered by spaCy.\n",
    "'''\n",
    "lemmas = [f\"Token: {token}, lemma: {token.lemma_}\" for token in filtered_tokens]\n",
    "\n",
    "'''\n",
    "Vectorization is a process that transforms a token into a vector, or a numeric array that, in the context of NLP, \n",
    "is unique to and represents various features of a token. \n",
    "Vectors are used under the hood to find word similarities, classify text, and perform other NLP operations.\n",
    "'''\n",
    "# filtered_tokens[4].vector\n",
    "'''\n",
    "Using Machine Learning Classifiers to Predict Sentiment:\n",
    "Once you have your vectorized data, a basic workflow for classification looks like this:\n",
    "\n",
    "Split your data into training and evaluation sets.\n",
    "Select a model architecture.\n",
    "Use training data to train your model.\n",
    "Use test data to evaluate the performance of your model.\n",
    "Use your trained model on new data to generate predictions, which in this case will be a number between -1.0 and 1.0.\n",
    "'''\n",
    "#Loading and Preprocessing Data\n",
    "import os\n",
    "import random\n",
    "\n",
    "def load_training_data(\n",
    "    data_directory: str = \"aclImdb/train\",\n",
    "    split: float = 0.8,\n",
    "    limit: int = 0\n",
    ") -> tuple:\n",
    "    # Load from files\n",
    "    reviews = []\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        labeled_directory = f\"{data_directory}/{label}\"\n",
    "        for review in os.listdir(labeled_directory):\n",
    "            if review.endswith(\".txt\"):\n",
    "                with open(f\"{labeled_directory}/{review}\") as f:\n",
    "                    text = f.read()\n",
    "                    '''\n",
    "                    Since you have each review open at this point, it’s a good idea to replace the <br /> HTML \n",
    "                    tags in the texts with newlines and to use .strip() to remove all leading and trailing whitespace.\n",
    "                    '''\n",
    "                    text = text.replace(\"<br />\", \"\\n\\n\")\n",
    "                    #when the review is not empty\n",
    "                    if text.strip():\n",
    "                        #the file name as neg and pos defines if a review is a negative or positive\n",
    "                        spacy_label = {\n",
    "                            \"cats\": {\n",
    "                                \"pos\": \"pos\" == label,\n",
    "                                \"neg\": \"neg\" == label\n",
    "                            }\n",
    "                        }\n",
    "                        reviews.append((text, spacy_label))\n",
    "    '''\n",
    "    After loading the files, you want to shuffle them. \n",
    "    This works to eliminate any possible bias from the order in which training data is loaded. \n",
    "    Since the random module makes this easy to do in one line, you’ll also see how to split your shuffled data:\n",
    "    '''\n",
    "    random.shuffle(reviews)\n",
    "\n",
    "    if limit:\n",
    "        reviews = reviews[:limit]\n",
    "    split = int(len(reviews) * split)\n",
    "    return reviews[:split], reviews[split:]\n",
    "\n",
    "# train, test = load_training_data(data_directory = \"/Users/yongpengfu/Desktop/aclImdb/train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456057c2",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Using VADER\n",
    "This is a tutorial for sentiment analysis using VADER. https://www.analyticsvidhya.com/blog/2022/10/sentiment-analysis-using-vader/#:~:text=Sentiment%20analysis%20is%20used%20to,positive%2C%20negative%2C%20or%20neutral.\n",
    "\n",
    "VADER( Valence Aware Dictionary for Sentiment Reasoning) is an NLTK module that provides sentiment scores based on the words used. It is a rule-based sentiment analyzer in which the terms are generally labeled as per their semantic orientation as either positive or negative.\n",
    "\n",
    "Terms:\n",
    "- The compound score is the sum of positive, negative & neutral scores which is then normalized between -1(most extreme negative) and +1 (most extreme positive). The more Compound score closer to +1, the higher the positivity of the text (https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/)\n",
    "- Polarity_scores: This function returns the sentiment strength based on the given input statement/text. For example: \n",
    "    - text= \"Bobby is an amazing guy\" \n",
    "    - sia.polarity_scores(text) \n",
    "    - {‘compound’: 0.5859, ‘neg’: 0.0, ‘neu’: 0.513, ‘pos’: 0.487} \n",
    "    - You can observe that the above statement is neutral\n",
    "    \n",
    "Advantages:  \n",
    "- The incredible thing about VADER is it doesn’t require a great deal of preprocessing to work. Unlike with some supervised methods of NLP, preprocessing necessities such as tokenisation and stemming/lemmatisation are not required. You can pretty much plug in any body of text and it will determine the sentiment.\n",
    "\n",
    "- VADER is even smart enough to understand the valence of non-conventional text, including emojis (i.e. :-( ), capitalisation (i.e. sad vs SAD) and extended punctuation (i.e. ? vs ???). This is what makes the module so good at analysing social media text. Additionally, VADER removes stop words automatically so there is no need to do so yourself.\n",
    "\n",
    "Disadvantages:  \n",
    "- There are also some disadvantages to this approach: Misspellings and grammatical mistakes may cause the analysis to overlook important words or usage. Sarcasm and irony may be misinterpreted. Analysis is language-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "512a899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/yongpengfu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>date</th>\n",
       "      <th>verified</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>helpfulVotes</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Janet</td>\n",
       "      <td>3</td>\n",
       "      <td>October 11, 2005</td>\n",
       "      <td>False</td>\n",
       "      <td>Def not best, but not worst</td>\n",
       "      <td>I had the Samsung A600 for awhile which is abs...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'neg': 0.08, 'neu': 0.816, 'pos': 0.105, 'com...</td>\n",
       "      <td>0.8629</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.080</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Luke Wyatt</td>\n",
       "      <td>1</td>\n",
       "      <td>January 7, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Text Messaging Doesn't Work</td>\n",
       "      <td>Due to a software issue between Nokia and Spri...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>{'neg': 0.02, 'neu': 0.876, 'pos': 0.104, 'com...</td>\n",
       "      <td>0.8860</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.020</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>Brooke</td>\n",
       "      <td>5</td>\n",
       "      <td>December 30, 2003</td>\n",
       "      <td>False</td>\n",
       "      <td>Love This Phone</td>\n",
       "      <td>This is a great, reliable phone. I also purcha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>{'neg': 0.051, 'neu': 0.846, 'pos': 0.103, 'co...</td>\n",
       "      <td>0.7992</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.051</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0000SX2UC</td>\n",
       "      <td>amy m. teague</td>\n",
       "      <td>3</td>\n",
       "      <td>March 18, 2004</td>\n",
       "      <td>False</td>\n",
       "      <td>Love the Phone, BUT...!</td>\n",
       "      <td>I love the phone and all, because I really did...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.844, 'pos': 0.156, 'comp...</td>\n",
       "      <td>0.9592</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.000</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin           name  rating               date  verified  \\\n",
       "0  B0000SX2UC          Janet       3   October 11, 2005     False   \n",
       "1  B0000SX2UC     Luke Wyatt       1    January 7, 2004     False   \n",
       "2  B0000SX2UC         Brooke       5  December 30, 2003     False   \n",
       "3  B0000SX2UC  amy m. teague       3     March 18, 2004     False   \n",
       "\n",
       "                         title  \\\n",
       "0  Def not best, but not worst   \n",
       "1  Text Messaging Doesn't Work   \n",
       "2              Love This Phone   \n",
       "3      Love the Phone, BUT...!   \n",
       "\n",
       "                                                body  helpfulVotes  \\\n",
       "0  I had the Samsung A600 for awhile which is abs...           1.0   \n",
       "1  Due to a software issue between Nokia and Spri...          17.0   \n",
       "2  This is a great, reliable phone. I also purcha...           5.0   \n",
       "3  I love the phone and all, because I really did...           1.0   \n",
       "\n",
       "                                              scores  compound    pos    neg  \\\n",
       "0  {'neg': 0.08, 'neu': 0.816, 'pos': 0.105, 'com...    0.8629  0.105  0.080   \n",
       "1  {'neg': 0.02, 'neu': 0.876, 'pos': 0.104, 'com...    0.8860  0.104  0.020   \n",
       "2  {'neg': 0.051, 'neu': 0.846, 'pos': 0.103, 'co...    0.7992  0.103  0.051   \n",
       "3  {'neg': 0.0, 'neu': 0.844, 'pos': 0.156, 'comp...    0.9592  0.156  0.000   \n",
       "\n",
       "  type  \n",
       "0  POS  \n",
       "1  POS  \n",
       "2  POS  \n",
       "3  POS  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "POS        46034\n",
       "NEG        13698\n",
       "NEUTRAL     8254\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In this exercise, I will use a CSV file containing reviews for different products. The link for the file is :\n",
    "# https://drive.google.com/file/d/1NYdZoMJvBWuCejMX28pVRVfMyOe1GhnZ/view?usp=sharing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#download vader from nltk\n",
    "#the data is doloaded in /Users/yongpengfu/nltk_data\n",
    "#you can change the directory following https://www.nltk.org/data.html\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#creating an object of sentiment intensity analyzer\n",
    "sia= SentimentIntensityAnalyzer()\n",
    "#reading csv file\n",
    "df = pd.read_csv('/Users/yongpengfu/Desktop/reviews.csv')\n",
    "# Let us now create a new column in our CSV file that stores the polarity scores of each review.\n",
    "#creating new column scores using polarity scores function\n",
    "df['scores']=df['body'].apply(lambda body: sia.polarity_scores(str(body)))\n",
    "df['compound']=df['scores'].apply(lambda score_dict:score_dict['compound'])\n",
    "df['pos']=df['scores'].apply(lambda pos_dict:pos_dict['pos'])\n",
    "df['neg']=df['scores'].apply(lambda neg_dict:neg_dict['neg'])\n",
    "#We then create a new column named type, which indicates whether the review is pos, neg, or neutral.\n",
    "df['type']=''\n",
    "df.loc[df.compound>0,'type']='POS'\n",
    "df.loc[df.compound==0,'type']='NEUTRAL'\n",
    "df.loc[df.compound<0,'type']='NEG'\n",
    "display(df.head(4))\n",
    "#Finally, we loop through the rows and count the total number of positive, negative, and neutral reviews.\n",
    "df.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10140351",
   "metadata": {},
   "source": [
    "### How To Perform Sentiment Analysis in Python 3 Using the Natural Language Toolkit (NLTK)\n",
    "Follow tutorial from https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "\n",
    "A large amount of data that is generated today is unstructured, which requires processing to generate insights. Some examples of unstructured data are news articles, posts on social media, and search history. The process of analyzing natural language and making sense out of it falls under the field of Natural Language Processing (NLP). Sentiment analysis is a common NLP task, which involves classifying texts or parts of texts into a pre-defined sentiment. You will use the Natural Language Toolkit (NLTK), a commonly used NLP library in Python, to analyze textual data.\n",
    "\n",
    "In this tutorial, you will prepare a dataset of sample tweets from the NLTK package for NLP with different data cleaning methods. Once the dataset is ready for processing, you will train a model on pre-classified tweets and use the model to classify the sample tweets into negative and positives sentiments.\n",
    "\n",
    "This article assumes that you are familiar with the basics of Python (see our How To Code in Python 3 series), primarily the use of data structures, classes, and methods. The tutorial assumes that you have no background in NLP and nltk, although some knowledge on it is an added advantage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4788f08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/yongpengfu/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "#use twitter sampels as tutorial\n",
    "#Step 1 — Installing NLTK and Downloading the Data\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55fd4be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yongpengfu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nHere, the .tokenized() method returns special characters such as @ and _. \\nThese characters will be removed through regular expressions later in this tutorial.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 2 — Tokenizing the Data\n",
    "'''\n",
    "Language in its original form cannot be accurately processed by a machine, \n",
    "so you need to process the language to make it easier for the machine to understand. \n",
    "The first part of making sense of the data is through a process called tokenization, \n",
    "or splitting strings into smaller parts called tokens.\n",
    "\n",
    "A token is a sequence of characters in text that serves as a unit. \n",
    "Based on how you create the tokens, they may consist of words, emoticons, hashtags, links, \n",
    "or even individual characters. A basic way of breaking language into tokens is by splitting the text \n",
    "based on whitespace and punctuation.\n",
    "'''\n",
    "from nltk.corpus import twitter_samples\n",
    "'''\n",
    "This will import three datasets from NLTK that contain various tweets to train and test the model:\n",
    "\n",
    "negative_tweets.json: 5000 tweets with negative sentiments\n",
    "positive_tweets.json: 5000 tweets with positive sentiments\n",
    "tweets.20150430-223406.json: 20000 tweets with no sentiments\n",
    "'''\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "'''\n",
    "Before using a tokenizer in NLTK, you need to download an additional resource, punkt. \n",
    "The punkt module is a pre-trained model that helps you tokenize words and sentences. \n",
    "For instance, this model knows that a name may contain a period (like “S. Daityari”) and \n",
    "the presence of this period in a sentence does not necessarily end it.\n",
    "'''\n",
    "nltk.download('punkt')\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "'''\n",
    "Here, the .tokenized() method returns special characters such as @ and _. \n",
    "These characters will be removed through regular expressions later in this tutorial.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8d4b197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yongpengfu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/yongpengfu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/yongpengfu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('#FollowFriday', 'JJ'), ('@France_Inte', 'NNP'), ('@PKuchly57', 'NNP'), ('@Milipol_Paris', 'NNP'), ('for', 'IN'), ('being', 'VBG'), ('top', 'JJ'), ('engaged', 'VBN'), ('members', 'NNS'), ('in', 'IN'), ('my', 'PRP$'), ('community', 'NN'), ('this', 'DT'), ('week', 'NN'), (':)', 'NN')]\n",
      "\n",
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n",
      "\n",
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nYou will notice that the verb being changes to its root form, be, and the noun members changes to member. \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 3 — Normalizing the Data\n",
    "'''\n",
    "Words have different forms—for instance, “ran”, “runs”, and “running” are various forms of the same verb, \n",
    "“run”. Depending on the requirement of your analysis, \n",
    "all of these versions may need to be converted to the same form, “run”. \n",
    "Normalization in NLP is the process of converting a word to its canonical form.\n",
    "'''\n",
    "'''\n",
    "wordnet is a lexical database for the English language that helps the script determine the base word. \n",
    "You need the averaged_perceptron_tagger resource to determine the context of a word in a sentence.\n",
    "'''\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "'''\n",
    "Once downloaded, you are almost ready to use the lemmatizer. Before running a lemmatizer, \n",
    "you need to determine the context for each word in your text. \n",
    "This is achieved by a tagging algorithm, which assesses the relative position of a word in a sentence. \n",
    "'''\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import twitter_samples\n",
    "'''\n",
    "From the list of tags, here is the list of the most common items and their meaning:\n",
    "\n",
    "NNP: Noun, proper, singular\n",
    "NN: Noun, common, singular or mass\n",
    "IN: Preposition or conjunction, subordinating\n",
    "VBG: Verb, gerund or present participle\n",
    "VBN: Verb, past participle\n",
    "\n",
    "see more in https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "In general, if a tag starts with NN, the word is a noun and if it stars with VB, the word is a verb. \n",
    "'''\n",
    "print(pos_tag(tweet_tokens[0]))\n",
    "print()\n",
    "\n",
    "'''\n",
    "To incorporate this into a function that normalizes a sentence, \n",
    "you should first generate the tags for each token in the text, \n",
    "and then lemmatize each word using the tag.\n",
    "\n",
    "Lemmatize using WordNet’s built-in morphy function. \n",
    "Returns the input word unchanged if it cannot be found in WordNet.\n",
    "read more in https://www.nltk.org/api/nltk.stem.wordnet.html#nltk.stem.WordNetLemmatizer\n",
    "\n",
    "pos (str) – The Part Of Speech tag. \n",
    "Valid options are “n” for nouns, \n",
    "“v” for verbs, \n",
    "“a” for adjectives, \n",
    "“r” for adverbs,\n",
    "“s” for satellite adjectives.\n",
    "'''\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence\n",
    "print(tweet_tokens[0])\n",
    "print()\n",
    "print(lemmatize_sentence(tweet_tokens[0]))\n",
    "\n",
    "'''\n",
    "You will notice that the verb being changes to its root form, be, and the noun members changes to member. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac3a7429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yongpengfu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n",
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "#Step 4 — Removing Noise from the Data\n",
    "'''\n",
    "In this step, you will remove noise from the dataset. \n",
    "Noise is any part of the text that does not add meaning or information to data.\n",
    "\n",
    "For this specific project, we will deal the following noise:\n",
    "Hyperlinks - All hyperlinks in Twitter are converted to the URL shortener t.co. \n",
    "    Therefore, keeping them in the text processing would not add any value to the analysis.\n",
    "Twitter handles in replies - These Twitter usernames are preceded by a @ symbol, \n",
    "    which does not convey any meaning.\n",
    "Punctuation and special characters - While these often provide context to textual data, \n",
    "    this context is often difficult to process. For simplicity, \n",
    "    you will remove all punctuation and special characters from tweets.\n",
    "'''\n",
    "...\n",
    "\n",
    "import re, string\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        #remove link\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        #remove any @text\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "        #If we still have string left\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "'''\n",
    "In addition to this, you will also remove stop words using a built-in set of stop words in NLTK, \n",
    "which needs to be downloaded separately.\n",
    "'''\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "#tweet_tokens[0] is a list, Notice that the function removes all @ mentions, stop words, \n",
    "#and converts the words to lowercase.\n",
    "print(remove_noise(tweet_tokens[0], stop_words))\n",
    "\n",
    "#use the remove_noise() function to clean the positive and negative tweets.\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "# Compare both versions of the 500th tweet in the list before and after clean\n",
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c102a37",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'positive_cleaned_tokens_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m token\n\u001b[0;32m---> 12\u001b[0m all_pos_words \u001b[38;5;241m=\u001b[39m get_all_words(\u001b[43mpositive_cleaned_tokens_list\u001b[49m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mNow that you have compiled all words in the sample of tweets, \u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03myou can find out which are the most common words using the FreqDist class of NLTK. \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n",
      "\u001b[0;31mNameError\u001b[0m: name 'positive_cleaned_tokens_list' is not defined"
     ]
    }
   ],
   "source": [
    "#Step 5 — Determining Word Density\n",
    "'''\n",
    "The most basic form of analysis on textual data is to take out the word frequency. \n",
    "A single tweet is too small of an entity to find out the distribution of words, hence, \n",
    "the analysis of the frequency of words would be done on all positive tweets.\n",
    "'''\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "'''\n",
    "Now that you have compiled all words in the sample of tweets, \n",
    "you can find out which are the most common words using the FreqDist class of NLTK. \n",
    "'''\n",
    "from nltk import FreqDist\n",
    "'''\n",
    "The .most_common() method lists the words which occur most frequently in the data. \n",
    "From this data, you can see that emoticon entities form some of the most common parts of positive tweets\n",
    "'''\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "862c26eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(({'want': True, 'full': True, 'thing': True, ':)': True}, 'Positive'),\n",
       " ({'thanks': True, 'alex': True, ':-)': True}, 'Positive'))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6 — Preparing Data for the Model\n",
    "'''\n",
    "Sentiment analysis is a process of identifying an attitude of the author on a topic that is being written about. \n",
    "You will create a training data set to train a model. \n",
    "It is a supervised learning machine learning process, \n",
    "which requires you to associate each dataset with a “sentiment” for training. \n",
    "In this tutorial, your model will use the “positive” and “negative” sentiments.\n",
    "'''\n",
    "\n",
    "'''\n",
    "First, you will prepare the data to be fed into the model. \n",
    "You will use the Naive Bayes classifier in NLTK to perform the modeling exercise. \n",
    "Notice that the model requires not just a list of words in a tweet, \n",
    "but a Python dictionary with words as keys and True as values. \n",
    "The following function makes a generator function to change the format of the cleaned data.\n",
    "'''\n",
    "'''\n",
    "Add the following code to convert the tweets from a list of cleaned tokens to dictionaries with keys \n",
    "as the tokens and True as values. The corresponding dictionaries are stored in positive_tokens_for_model and \n",
    "negative_tokens_for_model.\n",
    "'''\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "'''\n",
    "This code attaches a Positive or Negative label to each tweet. \n",
    "It then creates a dataset by joining the positive and negative tweets.\n",
    "\n",
    "By default, the data contains all positive tweets followed by all negative tweets in sequence.\n",
    "When training the model, you should provide a sample of your data that does not contain any bias. \n",
    "To avoid bias, you’ve added code to randomly arrange the data using the .shuffle() method of random.\n",
    "\n",
    "Finally, the code splits the shuffled data into a ratio of 70:30 for training and testing, respectively. \n",
    "Since the number of tweets is 10000, you can use the first 7000 tweets from the shuffled dataset for training \n",
    "the model and the final 3000 for testing the model.\n",
    "\n",
    "In this step, you converted the cleaned tokens to a dictionary form, randomly shuffled the dataset, \n",
    "and split it into training and testing data.\n",
    "'''\n",
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]\n",
    "train_data[0], test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ff23f1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.996\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2049.8 : 1.0\n",
      "                      :) = True           Positi : Negati =    989.1 : 1.0\n",
      "                     sad = True           Negati : Positi =     58.4 : 1.0\n",
      "                follower = True           Positi : Negati =     22.4 : 1.0\n",
      "                  arrive = True           Positi : Negati =     20.0 : 1.0\n",
      "                     x15 = True           Negati : Positi =     18.1 : 1.0\n",
      "                 awesome = True           Positi : Negati =     15.8 : 1.0\n",
      "                    poor = True           Negati : Positi =     14.8 : 1.0\n",
      "                 welcome = True           Positi : Negati =     14.3 : 1.0\n",
      "                     ugh = True           Negati : Positi =     14.2 : 1.0\n",
      "None\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "#Step 7 — Building and Testing the Model\n",
    "'''\n",
    "Finally, you can use the NaiveBayesClassifier class to build the model. \n",
    "Use the .train() method to train the model and the .accuracy() method to test the model on the testing data.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Accuracy is defined as the percentage of tweets in the testing dataset for which the model was correctly able to predict the sentiment.\n",
    "A 99.5% accuracy on the test set is pretty good.\n",
    "\n",
    "In the table that shows the most informative features, \n",
    "every row in the output shows the ratio of occurrence of a token in positive and negative tagged tweets in the training dataset. \n",
    "The first row in the data signifies that in all tweets containing the token :(, the ratio of negative to positives tweets was 2085.6 to 1. \n",
    "Interestingly, it seems that there was one token with :( in the positive datasets. You can see that the top two \n",
    "discriminating items in the text are the emoticons. Further, \n",
    "words such as sad lead to negative sentiments, whereas welcome and glad are associated with positive sentiments.\n",
    "'''\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))\n",
    "\n",
    "'''\n",
    "Next, you can check how the model performs on random tweets from Twitter. Add this code to the file:\n",
    "'''\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1629141051a6d5155c57bd565d322387c500daae99dfac9d1309d017b387cb02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
